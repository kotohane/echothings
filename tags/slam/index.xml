<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SLAM on Echo Things</title>
    <link>/tags/slam/</link>
    <description>Recent content in SLAM on Echo Things</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Tue, 15 Sep 2020 23:29:24 +0800</lastBuildDate>
    
	<atom:link href="/tags/slam/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>(Proj)机器人运行状态检测系统</title>
      <link>/posts/proj-spmat/</link>
      <pubDate>Tue, 15 Sep 2020 23:29:24 +0800</pubDate>
      
      <guid>/posts/proj-spmat/</guid>
      <description>关于项目 名称  机器人运行状态检测系统 Feature Exaction and Motion Estimation in Robot Navigation  地址 kotohane/spmat
想法来源 大二的时候在实验室的各种学习中（当时觉得学的太杂了什么都没学会，现在觉得当时学的东西太有用了，只恨自己学的太少了），学了SLAM和深度学习这两个基本没有交集的东西。 SLAM一般用于稠密建图，深度学习一般用来pattern recognition，各自都有很强、很专用的应用领域，但在这个领域之外一般很少能用得到，就算能用得到也是蹭一蹭热度，不起核心作用。
当时就在想有没有可能吧这两个东西结合起来使用，可以使用的没有那么紧密。正好，当时在做驾驶员疲劳度项目，因为没有什么思路，又是强商业化的领域，自己一个人做极度缺少数据（或者说缺少资金），觉得没有什么前途，就自己开了这个项目。
一般使用视觉对于一个机器进行识别（伺服之类的）都是使用外部的摄像头，然后对机器进行特征点提取等操作，在判断动作状态上不免会收到局限，因为目前在这种提取算法的情况下，识别的精度和准确度都不是很好，而把摄像头放在机械臂上，使用外部的特征点这个想法理论上会比原来的方法精确度更高，因为这些采取的特征点更多，有少量的误识别都可以自己纠正过来。
目标 判定机器人的运行状态（预设的）
总体思路 使用固定在机器人上的摄像头采集周围环境的SIFT特征点，进行特征点匹配，计算出每两帧之间的运动情况，再把一段时间内的运动情况放进神经网络里，训练出模型。在实际运行时就可以直接判断了。
成果 目前有一篇期刊的论文再投，已经修改过提交了，希望能中吧。
具体过程 1. 图像处理 先灰度化，然后再去模糊，由于提取SIFT特征点已经是一个比较成熟的算法了，这里不需要过于去追求这个速度。
机器一般被设定为是在室内，所以特征点会相当稠密，这时候应该设定好SIFT特征点采集的阈值，在检测速度和检测到的特征点数量之间达成一个平衡。
2. 对极约束（epipolar） 2D-2D点的匹配，求出本质矩阵（旋转+位移）。这个具体的公式我就不一一说了，推导起来不是很复杂。
这方面具体的内容可以见《高翔SLAM24讲》以及Courcera里UPenn关于Robotics的Specialization。
3. 数据整理 这一步已经把每两帧之间的运动变化求出来了。这里注意一点，由于2D的尺度特性，位移只能确定方向而无法确定具体的长度。下一步就是如何利用时间信息。
由于每两帧之间的时间间隔（或者说是帧率）不同，所以对时间轴进行对齐是有必要的。进行对齐后再把一段时间内的运动状态全部合起来，成为一个向量，这个向量将会作为接下来神经网络的输入。
这里引入了2个系统参数，时间的最小单位以及使用多少个这样的单位进行判断。
4. 训练 神经网络不能用的太复杂，输入只有50维。这里采用的就是最简单的网络，1个input，1个output，2个hidden layer。
由于网络相对来说比较简单，训练过程需要的数据量不是很复杂，一个人就足以采集。
5. 实时判断 至此，整个工程已经基本完成了，判断的准确度出乎意料的还是挺高的。目前测试下来4个动作的识别准确的95%+是没有什么问题的。
工程细节  对于特征点提取方面使用C++，而网络训练方面采用python（pytorch框架）。 系统参数有「相机内参」、「SIFT特征点阈值」、「时间的最小单位」、「使用多少个这样的单位」，采用配置文件的方式进行配置。  </description>
    </item>
    
  </channel>
</rss>